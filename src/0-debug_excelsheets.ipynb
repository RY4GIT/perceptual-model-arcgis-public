{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts to check the datasheet mis-entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"G:\\Shared drives\\Perceptual model review\\ForRyoko\\data\"\n",
    "model_type = \"Figure\" # Model type to check: Text or Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc = pd.read_excel(os.path.join(data_dir, 'Location_formatted.xlsx')) # The lat/lon should be pre-formatted in decimal units\n",
    "df_model = pd.read_excel(os.path.join(data_dir, f'ModelAnalysis_{model_type}.xlsx'))\n",
    "df_taxonomy = pd.read_excel(os.path.join(data_dir, 'ProcessHierarchyNetwork.xlsx'))\n",
    "df_FunctionType = pd.read_excel(os.path.join(data_dir, 'FunctionType.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse dataframe into table\n",
    "\n",
    "# Location table\n",
    "# Sanity check if data can be joined\n",
    "# df.set_index('key').join(other.set_index('key'))\n",
    "df_loc[\"id\"] = df_loc.index + 1\n",
    "df_loc = df_loc.drop(columns='Unnamed: 0')\n",
    "df_loc[\"huc_watershed_id\"] = np.nan\n",
    "\n",
    "# FunctionType table\n",
    "df_FunctionType[\"id\"] = df_FunctionType.index + 1\n",
    "\n",
    "# Citation table\n",
    "df_model[\"attribution_url\"].fillna(df_model[\"url\"], inplace=True)\n",
    "df_citation = df_model[[\"citation\", \"url\"]].copy()\n",
    "df_citation[\"attribution\"] = df_model[\"attribution\"].copy()\n",
    "df_citation[\"attribution_url\"] = df_model[\"attribution_url\"].copy()\n",
    "df_citation[\"id\"] = df_citation.index + 1\n",
    "\n",
    "# Spatial and temporal zone tables\n",
    "df_spatialZoneType = df_model[\"spatial_property\"].copy().drop_duplicates()\n",
    "df_spatialZoneType = df_spatialZoneType.to_frame()\n",
    "df_spatialZoneType.reset_index(inplace=True)\n",
    "df_spatialZoneType = df_spatialZoneType.drop(columns='index')\n",
    "df_spatialZoneType['id'] = df_spatialZoneType.index + 1\n",
    "df_temporalZoneType = df_model[\"temporal_property\"].copy().drop_duplicates()\n",
    "df_temporalZoneType = df_temporalZoneType.to_frame()\n",
    "df_temporalZoneType.reset_index(inplace=True)\n",
    "df_temporalZoneType = df_temporalZoneType.drop(columns='index')\n",
    "df_temporalZoneType['id'] = df_temporalZoneType.index + 1\n",
    "\n",
    "# Alternative name table\n",
    "df_altNames0 = df_taxonomy.set_index(['process', 'function', 'identifier', 'process_level']).apply(\n",
    "    lambda x: x.str.split(',').explode()).reset_index()\n",
    "df_altNames = df_altNames0[['alternative_names', 'process']].copy()\n",
    "df_altNames['alternative_names'] = df_altNames['alternative_names'].str.strip()\n",
    "df_altNames['alternative_names'] = df_altNames['alternative_names'].str.capitalize()\n",
    "df_altNames.dropna(axis=0, inplace=True)\n",
    "df_altNames[\"id\"] = df_altNames.index + 1\n",
    "\n",
    "# Model table\n",
    "df_model[\"id\"] = df_model.index + 1\n",
    "df_modelmain = df_model[['id', 'citation', 'watershed_name',\n",
    "                        'spatial_property', 'num_spatial_zones', 'temporal_property',\n",
    "                        'num_temporal_zones', 'vegetation_info', 'soil_info', 'geol_info',\n",
    "                        'topo_info', 'three_d_info', 'uncertainty_info', 'other_info'\n",
    "                        ]].copy()\n",
    "\n",
    "# LinkProcessPerceptual table\n",
    "# Get all the process original text and taxonomy name from model\n",
    "frames = [df_model[['id', 'flux1', 'flux1_taxonomy']].copy().rename(\n",
    "    columns={\"id\": \"entry_id\", \"flux1\": \"original_text\", \"flux1_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux2', 'flux2_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux2\": \"original_text\", \"flux2_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux3', 'flux3_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux3\": \"original_text\", \"flux3_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux4', 'flux4_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux4\": \"original_text\", \"flux4_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux5', 'flux5_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux5\": \"original_text\", \"flux5_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux6', 'flux6_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux6\": \"original_text\", \"flux6_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux7', 'flux7_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux7\": \"original_text\", \"flux7_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux8', 'flux8_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux8\": \"original_text\", \"flux8_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux9', 'flux9_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux9\": \"original_text\", \"flux9_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux10', 'flux10_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux10\": \"original_text\", \"flux10_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux11', 'flux11_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux11\": \"original_text\", \"flux11_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux12', 'flux12_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux12\": \"original_text\", \"flux12_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux13', 'flux13_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux13\": \"original_text\", \"flux13_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux14', 'flux14_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux14\": \"original_text\", \"flux14_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store1', 'store1_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store1\": \"original_text\", \"store1_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store2', 'store2_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store2\": \"original_text\", \"store2_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store3', 'store3_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store3\": \"original_text\", \"store3_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store4', 'store4_taxonomy']].copy().rename( columns={\"id\": \"entry_id\", \"store4\": \"original_text\", \"store4_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store5', 'store5_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store5\": \"original_text\", \"store5_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store6', 'store6_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store6\": \"original_text\", \"store6_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store7', 'store7_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store7\": \"original_text\", \"store7_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store8', 'store8_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store8\": \"original_text\", \"store8_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store9', 'store9_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store9\": \"original_text\", \"store9_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store10', 'store10_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store10\": \"original_text\", \"store10_taxonomy\": \"process\"})\n",
    "        ]\n",
    "\n",
    "df_linkProcessPerceptual0 = pd.concat(frames, axis=0, ignore_index=True)\n",
    "df_linkProcessPerceptual0[\"id\"] = df_linkProcessPerceptual0.index + 1\n",
    "\n",
    "# Create taxonomy table\n",
    "df_process0 = df_taxonomy.drop(columns='alternative_names')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check process taxonomy - flux and store names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>process_x</th>\n",
       "      <th>id</th>\n",
       "      <th>process_lower</th>\n",
       "      <th>process_y</th>\n",
       "      <th>function</th>\n",
       "      <th>identifier</th>\n",
       "      <th>process_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [entry_id, original_text, process_x, id, process_lower, process_y, function, identifier, process_level]\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# join process taxonomy and model table\n",
    "df_linkProcessPerceptual0[\"process_lower\"] = df_linkProcessPerceptual0['process'].str.lower()\n",
    "df_linkProcessPerceptual0[\"process_lower\"] = df_linkProcessPerceptual0['process_lower'].str.strip()\n",
    "df_process0[\"process_lower\"] = df_process0['process'].str.lower()\n",
    "df_process0[\"process_lower\"] = df_process0['process_lower'].str.strip()\n",
    "\n",
    "# find and add some new process from model table to taxonomy table (# Check here if you want to check process miscategorization)\n",
    "df_linkProcessPerceptual1 = df_linkProcessPerceptual0.merge(df_process0, on='process_lower', how='left')\n",
    "new_process = df_linkProcessPerceptual1.loc[(df_linkProcessPerceptual1['process_x'].isnull() == False) & (\n",
    "            df_linkProcessPerceptual1['process_y'].isnull() == True)]\n",
    "new_process.drop_duplicates(subset='process_lower', inplace=True)\n",
    "# new_process.to_excel(r'..\\data\\text_models_workspace\\newprocess_v2.xlsx')\n",
    "\n",
    "new_process\n",
    "\n",
    "# Returns empty dataframe if everything matches \n",
    "# If not, check back the datasheet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check match with location database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_x</th>\n",
       "      <th>citation</th>\n",
       "      <th>watershed_name</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>id_y</th>\n",
       "      <th>huc_watershed_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id_x, citation, watershed_name, name, lat, lon, area_km2, id_y, huc_watershed_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join location  and model table\n",
    "df_linkLocation = pd.merge(df_model[['id', 'citation', 'watershed_name']], df_loc, left_on='watershed_name', right_on='name', how='left')\n",
    "df_linkLocation.loc[(df_linkLocation['watershed_name'].isnull()) | (df_linkLocation['name'].isnull())]\n",
    "\n",
    "# Returns empty dataframe if everything matches \n",
    "# If not, check back the datasheet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Cropland described', 'Vegetation described',\n",
       "       'Forest described', 'Seasonal change discussed',\n",
       "       'Vegetation icons', 'Vegetation types described'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['vegetation_info'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Horizons described', 'Soil types described'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['soil_info'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Geology described', 'Glacier described', 'Bedrock described'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['geol_info'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Slopes described', 'N', 'Topography described', 'Scale bar shown'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['topo_info'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Unknown items identified', 'Uncertainty described'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['uncertainty_info'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Soil properties (surface sealing, cracking)',\n",
       "       'This is a longitudinal profile, average transit time given, scale given',\n",
       "       'Riparian area only',\n",
       "       'Show cross-section of water table slopes/depths for wet and dry seasons',\n",
       "       'Reaction fronts shown', nan,\n",
       "       'Joints/faults show, approximate scale given in caption',\n",
       "       'Soil clay content in caption, % streamflow contribution in legend, open boundary noted, extra notes shown on figures',\n",
       "       'The sources (=stores) and flow paths are shown as separate pictures. The diffuse groundwater flows are shown as being from shallow and deep aquifers, but are described as being \"flows through the soil matrix and macropores\"',\n",
       "       'Inconsistencies betweeen picture and legend, wiggly arrows meaning unknown, many unlabelled arrows',\n",
       "       'Arrows show water table rise/fall; indicates which storages have variable storage',\n",
       "       'Describes water sources as event water, shallow gw, deep gw',\n",
       "       'Focused on karst systems',\n",
       "       'Complex flow paths and changes in saturated zone shape/layers shown in diagrams, ',\n",
       "       'Just shown as soil sections',\n",
       "       'Focused on isotope enrichment processes',\n",
       "       'Arrows show direction of groundwater e.g. towards zones of higher conductivity',\n",
       "       'Pictures of suface rocks shown, unsure of significance',\n",
       "       'Also shows translation into model structure',\n",
       "       'Shows vertical scale, shows isolines of chloride for salinity',\n",
       "       'Bedrock bedding planes shown',\n",
       "       'Shows locations of new and old water',\n",
       "       'Soil property (organic/mineral)',\n",
       "       '?Should precipitation, evaporation count as fluxes?',\n",
       "       'Lots of information in text but very hard to match to unlabelled arrows in diagram',\n",
       "       'Shading used to show saturation level of soil, location of infiltration/recharge under depression shown by location of arrows',\n",
       "       'Shows different rainfall rates over time',\n",
       "       'Water ages were shown from 3 weeks - 6 months',\n",
       "       'Mostly focused on nitrates',\n",
       "       'Depth of surface layer marked on hillslope and close to channel',\n",
       "       'Also shows fluxes of DOC, TDS, and inorganic nitrogen',\n",
       "       'Soil property (free-draining)',\n",
       "       'Shows lateral and vertical scale, shows zones of different N-4 in soil',\n",
       "       'Shows scale in vertical and horizontal, figure shows longitudinal profile',\n",
       "       'Runoff components grouped as near surface runoff, shallow groundwater, deep groundwater',\n",
       "       'Unusual - plan view where location of different dominant processes is mapped, scale shown',\n",
       "       'Areas of slopes/wetlands given',\n",
       "       'All stores are shown separately where they are dry or filled with water',\n",
       "       'Caption describes the measurement types used to infer each process',\n",
       "       'Shows speed of process by arrow length, shows which process knowledge gained by which catchment investigation/data type',\n",
       "       'Also groups water bodies by their stable isotope composition',\n",
       "       'Flux property: temporary/permanent; cross-sectional slope of water table shown; vegetation leaf on/off shown; flows shown as mediated by storage or soil structure',\n",
       "       'Shows event/pre-event water fractions',\n",
       "       'Shows bedrock fractures, shows thickness of sediment',\n",
       "       'Bedding planes and fractures shown', 'N',\n",
       "       'Size of arrows determines the importance of flowpaths',\n",
       "       'Saturated hydraulic conductivity estimates shown',\n",
       "       'Arrow width represents flow path importance', 'Sediment transfer',\n",
       "       'Scale bar', 'Scale bar ', 'Salinity study'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['other_info'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Hillslope position', 'Catchment spatial scale', 'Topography',\n",
       "       'Aspects', 'Soil or Geology', 'Process', 'Land use / Land cover'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['spatial_property'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Season', 'N', 'Rainfall intensity', 'Season with snow', 'Wetness',\n",
       "       'Event', 'Season and wetness', 'Interannual'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['temporal_property'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(TODO) Write code to check if Taxonomy is not empty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3140ae756112501f18b23ec79a6bbbb54e715b030c96600dfefb60d12a8f240f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
