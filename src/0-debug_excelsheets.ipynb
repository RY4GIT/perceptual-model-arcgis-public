{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts to check the datasheet mis-entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code twice for each `model_type = \"Text\"` and `\"Figure\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# CHANGE HERE FOR YOUR PURPOSE ####################\n",
    "data_dir = f\"..\\data\"\n",
    "model_type = \"Figure\" # Model type to check: Text or Figure\n",
    "###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loc = pd.read_csv(os.path.join(data_dir, 'Location.csv')) # The lat/lon should be pre-formatted in decimal units\n",
    "df_model = pd.read_csv(os.path.join(data_dir, f'ModelAnalysis_{model_type}.csv'))\n",
    "df_taxonomy = pd.read_csv(os.path.join(data_dir, 'ProcessHierarchyNetwork.csv'))\n",
    "df_FunctionType = pd.read_csv(os.path.join(data_dir, 'FunctionType.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse dataframe into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location table\n",
    "# Sanity check if data can be joined\n",
    "# df.set_index('key').join(other.set_index('key'))\n",
    "df_loc[\"id\"] = df_loc.index + 1\n",
    "df_loc[\"huc_watershed_id\"] = np.nan\n",
    "\n",
    "# FunctionType table\n",
    "df_FunctionType[\"id\"] = df_FunctionType.index + 1\n",
    "\n",
    "# Citation table\n",
    "df_model[\"attribution_url\"].fillna(df_model[\"url\"], inplace=True)\n",
    "df_citation = df_model[[\"citation\", \"url\"]].copy()\n",
    "df_citation[\"attribution\"] = df_model[\"attribution\"].copy()\n",
    "df_citation[\"attribution_url\"] = df_model[\"attribution_url\"].copy()\n",
    "df_citation[\"id\"] = df_citation.index + 1\n",
    "\n",
    "# Spatial and temporal zone tables\n",
    "df_spatialZoneType = df_model[\"spatial_property\"].copy().drop_duplicates()\n",
    "df_spatialZoneType = df_spatialZoneType.to_frame()\n",
    "df_spatialZoneType.reset_index(inplace=True)\n",
    "df_spatialZoneType = df_spatialZoneType.drop(columns='index')\n",
    "df_spatialZoneType['id'] = df_spatialZoneType.index + 1\n",
    "df_temporalZoneType = df_model[\"temporal_property\"].copy().drop_duplicates()\n",
    "df_temporalZoneType = df_temporalZoneType.to_frame()\n",
    "df_temporalZoneType.reset_index(inplace=True)\n",
    "df_temporalZoneType = df_temporalZoneType.drop(columns='index')\n",
    "df_temporalZoneType['id'] = df_temporalZoneType.index + 1\n",
    "\n",
    "# Alternative name table\n",
    "df_altNames0 = df_taxonomy.set_index(['process', 'function', 'identifier', 'process_level']).apply(\n",
    "    lambda x: x.str.split(',').explode()).reset_index()\n",
    "df_altNames = df_altNames0[['alternative_names', 'process']].copy()\n",
    "df_altNames['alternative_names'] = df_altNames['alternative_names'].str.strip()\n",
    "df_altNames['alternative_names'] = df_altNames['alternative_names'].str.capitalize()\n",
    "df_altNames.dropna(axis=0, inplace=True)\n",
    "df_altNames[\"id\"] = df_altNames.index + 1\n",
    "\n",
    "# Model table\n",
    "df_model[\"id\"] = df_model.index + 1\n",
    "df_modelmain = df_model[['id', 'citation', 'watershed_name',\n",
    "                        'spatial_property', 'num_spatial_zones', 'temporal_property',\n",
    "                        'num_temporal_zones', 'vegetation_info', 'soil_info', 'geol_info',\n",
    "                        'topo_info', 'three_d_info', 'uncertainty_info', 'other_info'\n",
    "                        ]].copy()\n",
    "\n",
    "# LinkProcessPerceptual table\n",
    "# Get all the process original text and taxonomy name from model\n",
    "frames = [df_model[['id', 'flux1', 'flux1_taxonomy']].copy().rename(\n",
    "    columns={\"id\": \"entry_id\", \"flux1\": \"original_text\", \"flux1_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux2', 'flux2_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux2\": \"original_text\", \"flux2_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux3', 'flux3_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux3\": \"original_text\", \"flux3_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux4', 'flux4_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux4\": \"original_text\", \"flux4_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux5', 'flux5_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux5\": \"original_text\", \"flux5_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux6', 'flux6_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux6\": \"original_text\", \"flux6_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux7', 'flux7_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux7\": \"original_text\", \"flux7_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux8', 'flux8_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux8\": \"original_text\", \"flux8_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux9', 'flux9_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux9\": \"original_text\", \"flux9_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux10', 'flux10_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux10\": \"original_text\", \"flux10_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux11', 'flux11_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux11\": \"original_text\", \"flux11_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux12', 'flux12_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux12\": \"original_text\", \"flux12_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux13', 'flux13_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux13\": \"original_text\", \"flux13_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'flux14', 'flux14_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"flux14\": \"original_text\", \"flux14_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store1', 'store1_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store1\": \"original_text\", \"store1_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store2', 'store2_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store2\": \"original_text\", \"store2_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store3', 'store3_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store3\": \"original_text\", \"store3_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store4', 'store4_taxonomy']].copy().rename( columns={\"id\": \"entry_id\", \"store4\": \"original_text\", \"store4_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store5', 'store5_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store5\": \"original_text\", \"store5_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store6', 'store6_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store6\": \"original_text\", \"store6_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store7', 'store7_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store7\": \"original_text\", \"store7_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store8', 'store8_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store8\": \"original_text\", \"store8_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store9', 'store9_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store9\": \"original_text\", \"store9_taxonomy\": \"process\"}),\n",
    "        df_model[['id', 'store10', 'store10_taxonomy']].copy().rename(columns={\"id\": \"entry_id\", \"store10\": \"original_text\", \"store10_taxonomy\": \"process\"})\n",
    "        ]\n",
    "\n",
    "df_linkProcessPerceptual0 = pd.concat(frames, axis=0, ignore_index=True)\n",
    "df_linkProcessPerceptual0[\"id\"] = df_linkProcessPerceptual0.index + 1\n",
    "\n",
    "# Create taxonomy table\n",
    "df_process0 = df_taxonomy.drop(columns='alternative_names')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check process taxonomy - flux and store names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry_id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>process_x</th>\n",
       "      <th>id</th>\n",
       "      <th>process_lower</th>\n",
       "      <th>process_y</th>\n",
       "      <th>function</th>\n",
       "      <th>identifier</th>\n",
       "      <th>process_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [entry_id, original_text, process_x, id, process_lower, process_y, function, identifier, process_level]\n",
       "Index: []"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join process taxonomy and model table\n",
    "df_linkProcessPerceptual0[\"process_lower\"] = df_linkProcessPerceptual0['process'].str.lower()\n",
    "df_linkProcessPerceptual0[\"process_lower\"] = df_linkProcessPerceptual0['process_lower'].str.strip()\n",
    "df_process0[\"process_lower\"] = df_process0['process'].str.lower()\n",
    "df_process0[\"process_lower\"] = df_process0['process_lower'].str.strip()\n",
    "\n",
    "# find and add some new process from model table to taxonomy table (# Check here if you want to check process miscategorization)\n",
    "df_linkProcessPerceptual1 = df_linkProcessPerceptual0.merge(df_process0, on='process_lower', how='left')\n",
    "new_process = df_linkProcessPerceptual1.loc[(df_linkProcessPerceptual1['process_x'].isnull() == False) & (\n",
    "            df_linkProcessPerceptual1['process_y'].isnull() == True)]\n",
    "new_process.drop_duplicates(subset='process_lower', inplace=True)\n",
    "# new_process.to_excel(r'..\\data\\text_models_workspace\\newprocess_v2.xlsx')\n",
    "\n",
    "new_process\n",
    "\n",
    "# This should return empty dataframe if everything matches \n",
    "# If not, there was process names in 'ModelAnalysis_{model_type}.csv' that doesn't exist in 'ProcessHierarchyNetwork.csv'. Check back the datasheet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check match with location database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_x</th>\n",
       "      <th>citation</th>\n",
       "      <th>watershed_name</th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>id_y</th>\n",
       "      <th>huc_watershed_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_x citation watershed_name name country  lat  lon  area_km2  id_y  \\\n",
       "130   131      NaN            NaN  NaN     NaN  NaN  NaN       NaN   NaN   \n",
       "\n",
       "     huc_watershed_id  \n",
       "130               NaN  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join location  and model table\n",
    "df_linkLocation = pd.merge(df_model[['id', 'citation', 'watershed_name']], df_loc, left_on='watershed_name', right_on='name', how='left')\n",
    "df_linkLocation.loc[(df_linkLocation['watershed_name'].isnull()) | (df_linkLocation['name'].isnull())]\n",
    "\n",
    "# This should return empty dataframe if everything matches \n",
    "# If not, there was watershed name in 'ModelAnalysis_{model_type}.csv' that doesn't exist in 'Location_formatted.csv'. Check back the datasheet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check watershed attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Cropland described', 'Vegetation described',\n",
       "       'Forest described', 'Seasonal change discussed',\n",
       "       'Vegetation icons', 'Vegetation types described', nan],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['vegetation_info'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Soil described', 'Soil types described',\n",
       "       'Soil texture described', 'Soil hydraulic properties described',\n",
       "       'Multiple properties described', 'Horizons described'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['soil_info'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Geology described', 'Glacier described', 'Karst described',\n",
       "       'Bedrock described'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['geol_info'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Slopes described', 'Topography described'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['topo_info'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Multiple interpretations demonstrated',\n",
       "       'Limitations discussed', 'Uncertainty described',\n",
       "       'Unknown items identified'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['uncertainty_info'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Impact of root activity described', 'salinity study'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['other_info'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hillslope position', 'N', 'Land use / Land cover',\n",
       "       'Soil or Geology', 'Catchment spatial scale', 'Process',\n",
       "       'Hillslope position/Catchment spatial scale', 'Topography',\n",
       "       'Multiple catchments', 'Tributary',\n",
       "       'Land cover / Hillslope position'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['spatial_property'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Wetness and event', 'Season', 'Event', 'Season and event',\n",
       "       'Rainfall intensity', 'Season and rainfall intensity', 'Wetness',\n",
       "       'Season with snow', 'Interannual', 'Event and rainfall intensity',\n",
       "       'Season and rainfall intensity with snow', 'Season and wetness',\n",
       "       nan], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model['temporal_property'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcgis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3140ae756112501f18b23ec79a6bbbb54e715b030c96600dfefb60d12a8f240f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
